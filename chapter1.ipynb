{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一章节:介绍tensor 和 variable\n",
    "# https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/    中文文档\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([3])\n",
    "np.array([3],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3809, -0.3181, -0.3212],\n",
      "        [-0.2742, -2.1222,  0.9637]])\n"
     ]
    }
   ],
   "source": [
    "#级别:np->tensor->variable\n",
    "#variable.data=tensor tensor.data还是tensor    tensor.numpy()=np   variable没有.numpy()\n",
    "#1.np转tensor\n",
    "a=np.random.randn(2, 3)\n",
    "tensor1 = torch.Tensor(a)\n",
    "print(tensor1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类型 <class 'torch.Tensor'>\n",
      "维度: 2\n",
      "元素个数: <built-in method numel of Tensor object at 0x00000233AC9C0360>\n"
     ]
    }
   ],
   "source": [
    "print('类型',type(tensor1))\n",
    "print('维度:',tensor1.dim())\n",
    "print('元素个数:',tensor1.numel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3809438  -0.31814507 -0.3212034 ]\n",
      " [-0.27416033 -2.1222126   0.96373165]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "#2.tensor 转np\n",
    "b=tensor1.numpy()\n",
    "print(b)\n",
    "print(b.dtype)# np也有.dtype   tensor也有\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建立FloatTensor: tensor([3.])\n",
      "建立FloatTensor: tensor([-1105343071996124941766361088.0000,                             0.0000,\n",
      "                                    0.0000])\n",
      "xx3 tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[ 1.2397,  0.9212],\n",
      "        [-1.5091, -1.3504],\n",
      "        [ 1.2199,  1.5377]])\n",
      "torch.float32\n",
      "tensor([[ 1.2397,  0.9212],\n",
      "        [-1.5091, -1.3504],\n",
      "        [ 1.2199,  1.5377]], dtype=torch.float64)\n",
      "转float torch.float32\n",
      "tensor([[ 1.2397,  0.9212],\n",
      "        [-1.5091, -1.3504],\n",
      "        [ 1.2199,  1.5377]])\n",
      "转long torch.int64\n",
      "tensor([[ 1,  0],\n",
      "        [-1, -1],\n",
      "        [ 1,  1]])\n"
     ]
    }
   ],
   "source": [
    "#3.  关于tensor\n",
    "#https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/\n",
    "\n",
    "#torch.FloatTensor()=torch.Tensor() 默认float32  可以放np, 也可以放int  [int]  list[[1,2],[3,4]]   torch.DoubleTensor() torch.IntTensor\n",
    "xx=torch.Tensor([3]) #定义一个数3,\n",
    "xx2=torch.Tensor(3) #定义3个常量\n",
    "print(\"建立FloatTensor:\",xx) \n",
    "print(\"建立FloatTensor:\",xx2)\n",
    "\n",
    "xx3=torch.Tensor(2,4).zero_()\n",
    "print('xx3',xx3)\n",
    "#print(xx3)\n",
    "\n",
    "#torch.ones(3,2) \n",
    "\n",
    "x = torch.randn(3, 2)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "print(x.type(torch.DoubleTensor))\n",
    "\n",
    "\n",
    "# tensor 转化类型  x是tensor    tensor.float()   .int()   .long()\n",
    "x=x.float()#浮点型32\n",
    "print('转float',x.dtype)\n",
    "print(x)\n",
    "x=x.long()#整形#.int()\n",
    "print('转long',x.dtype)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor的计算  (tensor适用的variable也适用)\n",
    "# 加法 +   \n",
    "#      -\n",
    "# torch.sum() 对整个矩阵求和，如果写上(S ,dim=0)，则沿着第0维度 对S矩阵 求和\n",
    "# torch.t() 转置\n",
    "# torch.mul(tensor1,tensor2) 对应元素相乘\n",
    "# torch.mm(tensor1,tensor2)  横乘以竖\n",
    "# \n",
    "torch.Tensor([3])+torch.Tensor([4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 .variable 变量\n",
    "# https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-autograd/\n",
    "# 自动求导\n",
    "\n",
    "# 从 tensor 转variable\n",
    "# Variable(tensor, requires_grad=True)   #如果不写grad参数，默认False，说明这个variable不需要求导，作为常数用\n",
    "#通常网络的输入数据就是false的variable     权重都是True的variable\n",
    "\n",
    "# 定义\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "# variable 属性\n",
    "# variable.squeeze()  把维度为1 的维度去掉\n",
    "# .view(3,-1) 维度转化用的\n",
    "# 注意使用维度,如果要建立三维的（最外边为时间维），(时间，i , j )  第0维就是最外的维度\n",
    "# .requires_grad 查询是否可导\n",
    "# .is_leaf 是否叶子变量\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# variable的求导问题\n",
    "# 注意variable.data.numpy() 假设变量A=3，我们这样只是取出了3，变量里面的值，而不是取变量\n",
    "# y=x^2+1  y.backward()   x.grad 求得y对x的导数\n",
    "# 自己用Variable()创建的都叫做leaf variable 这种变量最好不要修改，还有禁止inplace操作 x=x+1 inplace\n",
    "# 非leaf variable 指的是 (1)y=x+2  x是自己定义的variable y是凭空写的\n",
    "#                        (2)y=x.clone()   此时y则为非叶子   非叶子都是可以修改赋值的\n",
    "# 如果要改variable的值怎么办 参考   线性回归模型.py\n",
    "# w.data = w.data - 1e-2 * w.grad.data # 手动更新 w   w是定义的variable， 这样就只改variable的数字，而不是改varialbe  而且可以inplace操作\n",
    "\n",
    "# 如果想用高级的更新方法  如adam 等， 不想手动更新，则要这样定义W=nn.Parameter(tensor) b也是 然后写好数学公式  W就是variable\n",
    "#nn.para就是把变量绑定到类nn.Module上，创建对象net之后,用optimizer(net.parameters())，就能对建立的nn.para进行更新了\n",
    "\n",
    "# torch.mm(nn.Par,tensor)   第一参表示W，第二表示数据data\n",
    "# optimizer = torch.optim.SGD([w, b], 1e-1)   如果建立了模型就用这个net.parameters()代替[w,b]，这是一个对象，他的类继承了nn.Module \n",
    "# 就会更新net里面那些grad=True的variable\n",
    " \n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# loss = criterion(out, Variable(y))\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step() \n",
    "\n",
    "\n",
    "\n",
    "# 常用的两种下降策略\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "#optimizer = optim.Adam([var1, var2], lr = 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. 总结 np tensor variable\n",
    "#定义的时候(,dtype=float)  都可以定义类型\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "最大值： tensor([0, 0, 0])   最大下标: tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#5.按行按列计算\n",
    "print(x)\n",
    "max_value, max_idx = torch.max(x, dim=1)#按行，意思就是每行操作一次，取每行最大\n",
    "print('最大值：',max_value,'  最大下标:',max_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "sum_x = torch.sum(x, dim=1)#按行  toch.sum(variable1==variable2)统计两个variable一维矩阵中，相同的数量\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原维度 torch.Size([3, 2])\n",
      "增加维度: torch.Size([1, 3, 2])\n",
      "减少维度 torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "#增减维度\n",
    "x = torch.randn(3, 2)\n",
    "print('原维度',x.shape)\n",
    "x = x.unsqueeze(0) # 在第一维增加\n",
    "\n",
    "print('增加维度:',x.shape)\n",
    "x = x.squeeze()#减少维度只有1的维度，如果填入数，就指定下标的删除\n",
    "print('减少维度',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#重排列\n",
    "x = torch.randn(3, 2)\n",
    "x = x.permute(1, 0) # permute 可以重新排列 tensor 的维度\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交换维度: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = x.transpose(0, 1)  # transpose 交换 tensor 中的两个维度\n",
    "print('交换维度:',x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第二维reshape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2)\n",
    "\n",
    "#reshape操作\n",
    "x = x.view(-1) # -1 表示任意的大小，5 表示第二维变成 5\n",
    "print('第二维reshape:',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "求和: tensor([[ 0.3717, -0.7831,  0.8105, -2.9360],\n",
      "        [ 0.0410, -2.0129, -1.9873, -0.8350],\n",
      "        [ 0.6374,  0.9576, -0.1994,  0.3087]])\n",
      "修改前: \n",
      " tensor([[ 1.0996, -0.3549,  0.3767, -1.2068],\n",
      "        [-0.5463, -1.9779, -1.6327, -0.4493],\n",
      "        [-0.5359, -0.1588, -0.3218, -0.1045]])\n",
      "修改后: \n",
      " tensor([[ 1.0996, -0.3549,  0.3767, -1.2068],\n",
      "        [-0.5463,  2.0000,  2.0000, -0.4493],\n",
      "        [-0.5359,  2.0000,  2.0000, -0.1045]])\n"
     ]
    }
   ],
   "source": [
    "#6.变量之间计算\n",
    "x = torch.randn(3, 4)\n",
    "y = torch.randn(3, 4)\n",
    "z = x + y\n",
    "print('求和:',z)\n",
    "#如果是tensorflow，则要run才能运行\n",
    "\n",
    "#批量赋值\n",
    "print('修改前:','\\n',x)\n",
    "x[1:3, 1:3] = 2 #1到2的下标\n",
    "print('修改后:','\\n',x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
