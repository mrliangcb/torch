
# 第1章节:介绍tensor 和 variable  

参考:https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/   

```py
import numpy as np
import torch
from torch.autograd import Variable
```

### 1.tensor  
(1)为什么要有variable?  
variable才能求导。tensor是variable的数字内容，所以用tensor才能初始化variable，不能用np.  
(2).np,tensor,variable三者的关系  
- 递进关系，级别: np-> tensor-> variable  
```py 
np->tensor :   tensor1 = torch.Tensor(np)   
相反       :   tensor.numpy()=np  

tensor->variable  :  Variable(tensor, requires_grad=True) #默认为false，则不可求导变量
相反              :  variable.data=tensor

np.array([],dtype=float) #np.float64 只有np才能这样定义，tensor和var都不行

tensor属性:
print('类型',type(tensor))   
print('维度:',tensor.dim())  
print('元素个数:',tensor.numel)  
print('数据类型',tenor.dtype)  
len(tensor)

```

(3)创建tensor  
```py
torch.FloatTensor(np)=torch.Tensor(np)   #()里面: 2表示有两个数，[2]表示单一个数2，np.array([[2]])输入是多少维，tensor就是多少维度   默认float32
torch.Tensor(2,4).zero_()    #建立2行4列的tensor，用0填充
torch.IntTensor(2,4).zero_()
torch.randn(3, 2)   #3行2列随机数
tensor.type(torch.DoubleTensor)   #数据类型转为双精度float64
tensor.float()   #转为float32
tensor.long()    #转为整型
torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))
```
(4)运算  
注意，不同于tensorflow，这里的运算不用启动图，直接像python一样运算  
```py
#加法  减法类似
tensor+tensor  

#内部求和
torch.sum()   #整个矩阵求和
torch.sum(tensor ,dim=0)   #行求和

torch.t(tensor)  #转置

#乘法
torch.mul(tensor1,tensor2) #对应元素相乘
torch.mm(tensor1,tensor2)  #横乘以竖

torch.mean(tensor)  #求平均

# tensor批量赋值
tensor[1:3, 1:3] = 2

#绝对值
torch.abs(torch.FloatTensor([-1, -2, 3]))
```

### 2.变量variable  
(1)叶子节点的问题:  
var1=variable(tensor)   #创建的变量都是叶子变量  

- 最好不要直接给叶子节点赋值，如var1[1,1]=3,给这个元素赋3是不行的，所以中介var矩阵，是不设置require_grad的，用来存放这些可导元计算的值  
- 叶子节点不可以做inplace操作  如 var1=var1+1   正确的是var2=var1 而且var2跟var1一样是叶子和可导  
- var修改值，更新值: w.data = w.data - 0.001 * w.grad.data。#必须是求导之后释放了求导空间，才可以手动更新var里面的值
- 实际情况下，把数据打包成tensor(nograd)之后，与W(可导变量)运算，然后输出对W求导
- 给var赋值，一般是var的运算结果传给var，如果想要赋数字，则要var.data 
```py
y=var1**2+1
y.backward()  #y对所有可导元求导 并且y只能是一个数，不能是矩阵  var1可以是矩阵
var1.grad    #y对var求导得到的值


非叶子var=叶子var.clone()
var.is_leaf #判断是否叶子节点
var.requires_grad  #判断是否可导变量

```

(2)操作  
https://zhuanlan.zhihu.com/p/31495102
参考此维度操作
```py
variable.squeeze()  #把维度为1 的维度去掉
var.view(3,-1) 	#维度转化
(时间，i , j )  #常用参考 
# 广播 https://blog.csdn.net/weixin_39845112/article/details/79935254
tensor.expand(channel,) #在第0个位置上扩展，相当于在0位置增加一个维度，而且复制channel份  称为广播

max_value, max_idx = torch.max(x, dim=1)  #取出最大数，和下标,tensor型
var = var.unsqueeze(0) #增加第一维度
var.permute(1, 0) #重新排列维度
var.transpose(0, 1) #交换维度

torch.cat() torch.Tensor.expand()

torch.squeeze() 

torch.Tensor.repeat() tensor1.repeat(1,7)#填复制扩展之后的shape  跟expand的用法几乎一样，expand是复制张量，repeat是复制张量数据

torch.Tensor.narrow() torch.Tensor.view()

torch.Tensor.resize_() torch.Tensor.permute()


```




















