
# 第1章节:介绍tensor 和 variable  

参考:https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/   

```py
import numpy as np
import torch
from torch.autograd import Variable
```

### 1.tensor  
(1)为什么要有variable?  
variable才能求导。tensor是variable的数字内容，所以用tensor才能初始化variable，不能用np.  
(2).np,tensor,variable三者的关系  
- 递进关系，级别: np-> tensor-> variable  
```py 
np->tensor :   tensor1 = torch.Tensor(np)   
相反       :   tensor.numpy()=np  

tensor->variable  :  Variable(tensor, requires_grad=True) #默认为false，则不可求导变量
相反              :  variable.data=tensor

np.array([],dtype=float) #np.float64 只有np才能这样定义，tensor和var都不行

tensor属性:
print('类型',type(tensor))   
print('维度:',tensor.dim())  
print('元素个数:',tensor.numel)  
print('数据类型',tenor.dtype)  
len(tensor)

```

(3)创建tensor  
```py
torch.FloatTensor(np)=torch.Tensor(np)   #()里面: 2表示有两个数，[2]表示单一个数2，np.array([[2]])输入是多少维，tensor就是多少维度   默认float32
torch.Tensor(2,4).zero_()    #建立2行4列的tensor，用0填充
torch.Tensor(2)  #这个张量有两个数，随机初始
torch.Tensor([2]) #这个张量有一个数，初始为2
torch.IntTensor(2,4).zero_()
torch.randn(3, 2)   #3行2列随机数
tensor.type(torch.DoubleTensor)   #数据类型转为双精度float64
tensor.float()   #转为float32
tensor.long()    #转为整型
torch.normal(means=torch.arange(1, 11), std=torch.arange(1, 0, -0.1))
torch.ones(2,3)
torch.zeros_like(tensor1) #创建与tensor1完全同大小的矩阵
```
(4)运算  
注意，不同于tensorflow，这里的运算不用启动图，直接像python一样运算  
```py
#加法  减法类似
tensor+tensor  

#内部求和
torch.sum()   #整个矩阵求和
torch.sum(tensor ,dim=0)   #行求和

torch.t(tensor)  #转置

#乘法
torch.mul(tensor1,tensor2) #对应元素相乘
torch.mm(tensor1,tensor2)  #横乘以竖

torch.mean(tensor)  #求平均

# tensor批量赋值
tensor[1:3, 1:3] = 2

#绝对值
torch.abs(torch.FloatTensor([-1, -2, 3]))
```

### 2.变量variable  
(1)叶子节点的问题:  
var1=variable(tensor)   #创建的变量都是叶子变量  



- 最好不要直接给叶子节点赋值，如var1[1,1]=3,给这个元素赋3是不行的，所以中介var矩阵，是不设置require_grad的，用来存放这些可导元计算的值  
- 叶子节点不可以做inplace操作  如 var1=var1+1   正确的是var2=var1 而且var2跟var1一样是叶子和可导  
- var修改值，更新值: w.data = w.data - 0.001 * w.grad.data。#必须是求导之后释放了求导空间，才可以手动更新var里面的值，正向传值之前可以更改值，但之后就不可以
- w.data=np等, w=var，tensor等
实际情况下，把数据打包成tensor(nograd)之后，与W(可导变量)运算，然后输出对W求导
- 给var赋值，一般是var的运算结果传给var，如果想要赋数字，则要var.data 
```py
y=var1**2+1
y.backward()  #y对所有可导元求导 并且y只能是一个数，不能是矩阵  var1可以是矩阵
var1.grad    #y对var求导得到的值


非叶子var=叶子var.clone()
var.is_leaf #判断是否叶子节点
var.requires_grad  #判断是否可导变量

```

(2)操作  
https://zhuanlan.zhihu.com/p/31495102
参考此维度操作
```py
variable.squeeze()  #把维度为1 的维度去掉
var.view(3,-1) 	#维度转化
(时间，i , j )  #常用参考 
# 广播 https://blog.csdn.net/weixin_39845112/article/details/79935254
tensor.expand(channel,) #在第0个位置上扩展，相当于在0位置增加一个维度，而且复制channel份  称为广播

max_value, max_idx = torch.max(x, dim=1)  #取出最大数，和下标,tensor型
var = var.unsqueeze(0) #增加第一维度
var.permute(1, 0) #重新排列维度
var.transpose(0, 1) #交换维度

torch.cat() torch.Tensor.expand()

torch.squeeze() 

torch.Tensor.repeat() tensor1.repeat(1,7)#填复制扩展之后的shape  跟expand的用法几乎一样，expand是复制张量，repeat是复制张量数据
b.repeat(1,1,2,1)  #1指的是在该维度上放原来的维度 2是在那个维度上重复*2个   可以加维度不可以减围度，加的维度写1，也就是维持原来的0
#一个按列做normalize的例子
a=torch.Tensor([[[[1,2,3,4,5],[2,3,4,5,6]]],[[[1,2,3,4,5],[1,2,3,4,6]]]])
b=torch.sum(a,dim=2)
c=b.repeat(1,1,2,1)#2是按行求和之后，输出为1行，然后扩展成原来的行数
a/c


torch.Tensor.narrow()

torch.Tensor.view()  #view自由度很大，可以改变或者增加维度(90,90)的可以view成(1,1,90,90)

torch.Tensor.resize_() torch.Tensor.permute()


```




















